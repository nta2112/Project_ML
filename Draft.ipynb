{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/informer.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class ProbSparseAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=False, factor=5, scale=None, attention_dropout=0.1):\n",
    "        super(ProbSparseAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def _prob_QK(self, Q, K, sample_k, n_top):\n",
    "        B, H, L_Q, D = Q.shape\n",
    "        _, _, L_K, _ = K.shape\n",
    "\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, D)\n",
    "        index_sample = torch.randint(L_K, (L_Q, sample_k))\n",
    "        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n",
    "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n",
    "\n",
    "        M = Q_K_sample.max(-1)[0] - Q_K_sample.mean(-1)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "\n",
    "        return M_top\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L_Q, D = queries.shape\n",
    "        B, L_K, D = keys.shape\n",
    "        H = 8\n",
    "        D_head = D // H\n",
    "\n",
    "        queries = queries.view(B, L_Q, H, D_head).transpose(1, 2)\n",
    "        keys = keys.view(B, L_K, H, D_head).transpose(1, 2)\n",
    "        values = values.view(B, L_K, H, D_head).transpose(1, 2)\n",
    "\n",
    "        U_part = self.factor * math.ceil(math.log(L_K))\n",
    "        u = self.factor * math.ceil(math.log(L_Q))\n",
    "\n",
    "        scores_top = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)\n",
    "\n",
    "        Q_reduce = queries[:, :, scores_top, :]\n",
    "        scores = torch.matmul(Q_reduce, keys.transpose(-2, -1)) / math.sqrt(D_head)\n",
    "        A = self.dropout(torch.softmax(scores, dim=-1))\n",
    "        V = torch.matmul(A, values)\n",
    "\n",
    "        out = torch.zeros(B, H, L_Q, D_head, device=queries.device)\n",
    "        out[:, :, scores_top, :] = V\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L_Q, D)\n",
    "        return out, None\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_model)\n",
    "        self.key_projection = nn.Linear(d_model, d_model)\n",
    "        self.value_projection = nn.Linear(d_model, d_model)\n",
    "        self.out_projection = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        queries = self.query_projection(x)\n",
    "        keys = self.key_projection(x)\n",
    "        values = self.value_projection(x)\n",
    "        out, _ = self.inner_attention(queries, keys, values, attn_mask=None)\n",
    "        out = self.out_projection(out)\n",
    "        return self.norm(out + residual)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = AttentionLayer(attention, d_model, n_heads=8)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention(x)\n",
    "        res = x\n",
    "        x = self.ffn(x)\n",
    "        return self.norm(res + self.dropout(x))\n",
    "\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, input_len, d_model=512, e_layers=3, d_ff=2048):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Conv1d(1, d_model, kernel_size=3, padding=1)\n",
    "        self.encoder = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                ProbSparseAttention(), d_model=d_model, d_ff=d_ff\n",
    "            ) for _ in range(e_layers)\n",
    "        ])\n",
    "        self.projection = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, L, 1] => [B, d_model, L] => [B, L, d_model]\n",
    "        x = self.embedding(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        out = self.projection(x[:, -1, :])  # dá»± bÃ¡o Ä‘iá»ƒm cuá»‘i\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/data_loader.py\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def process_file(file_path, window_size=60):\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path, columns=['close'])\n",
    "        data = df['close'].dropna().values\n",
    "        if len(data) <= window_size:\n",
    "            return [], []\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - window_size):\n",
    "            X.append(data[i:i + window_size])\n",
    "            y.append(data[i + window_size])\n",
    "        return np.array(X), np.array(y)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Lá»—i khi xá»­ lÃ½ file {file_path}: {e}\")\n",
    "        return np.empty((0, window_size)), np.empty((0,))\n",
    "\n",
    "def load_batch_files(file_list, window_size=60):\n",
    "    X_all, y_all = [], []\n",
    "    for f in file_list:\n",
    "        X, y = process_file(f, window_size)\n",
    "        if len(X):\n",
    "            X_all.append(X)\n",
    "            y_all.append(y)\n",
    "\n",
    "    if not X_all:\n",
    "        return None\n",
    "\n",
    "    X_all = np.concatenate(X_all, axis=0)\n",
    "    y_all = np.concatenate(y_all, axis=0)\n",
    "\n",
    "    mean = X_all.mean()\n",
    "    std = X_all.std()\n",
    "    X_all = (X_all - mean) / std\n",
    "\n",
    "    X_all = np.expand_dims(X_all, axis=-1)  # [B, L, 1]\n",
    "\n",
    "    return TimeSeriesDataset(X_all, y_all), mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.informer import Informer\n",
    "from utils.data_loader import load_batch_files\n",
    "\n",
    "# ==== Cáº¥u hÃ¬nh ====\n",
    "DATA_DIR = \"/content/drive/MyDrive/Draft/TimeSeries/Data_50\"\n",
    "BATCH_SIZE = 5\n",
    "WINDOW_SIZE = 60\n",
    "EPOCHS_PER_BATCH = 1\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_SPLIT = 0.1\n",
    "\n",
    "files = glob.glob(os.path.join(DATA_DIR, \"*.parquet\"))\n",
    "total_batches = len(files) // BATCH_SIZE + int(len(files) % BATCH_SIZE != 0)\n",
    "\n",
    "model = None\n",
    "criterion = nn.MSELoss()\n",
    "history_all = {'train_loss': [], 'val_loss': []}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for batch_num in range(total_batches):\n",
    "    print(f\"\\nðŸ”„ Batch {batch_num+1}/{total_batches}\")\n",
    "    batch_files = files[batch_num * BATCH_SIZE:(batch_num + 1) * BATCH_SIZE]\n",
    "    dataset, mean, std = load_batch_files(batch_files, window_size=WINDOW_SIZE)\n",
    "\n",
    "    if dataset is None:\n",
    "        print(\"âš ï¸ Batch trá»‘ng, bá» qua.\")\n",
    "        continue\n",
    "\n",
    "    val_size = int(len(dataset) * VAL_SPLIT)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=TRAIN_BATCH_SIZE)\n",
    "\n",
    "    if model is None:\n",
    "        model = Informer(input_len=WINDOW_SIZE).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # ==== Huáº¥n luyá»‡n ====\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for x_batch, y_batch in tqdm(train_loader):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch).squeeze()\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * len(x_batch)\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # ==== Validation ====\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            pred = model(x_val).squeeze()\n",
    "            loss = criterion(pred, y_val)\n",
    "            val_loss += loss.item() * len(x_val)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    print(f\"âœ… Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "    history_all['train_loss'].append(train_loss)\n",
    "    history_all['val_loss'].append(val_loss)\n",
    "\n",
    "# ==== Váº½ biá»ƒu Ä‘á»“ loss ====\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_all['train_loss'], label='Train Loss')\n",
    "plt.plot(history_all['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Batch Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Loss qua cÃ¡c batch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_loss.png\")\n",
    "print(\"ðŸ“‰ ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ loss vÃ o training_loss.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
